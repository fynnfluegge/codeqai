[
  {
    "input": "\"\"\"\n    Initializes the necessary components for the application.\n\n    Args:\n        config (dict): Configuration dictionary containing settings for embeddings and LLM.\n        repo_name (str): The name of the repository.\n        embeddings_model (Embeddings, optional): Pre-initialized embeddings model. Defaults to None.\n\n    Returns:\n        tuple: A tuple containing the vector store, memory, and QA chain.\n    \"\"\"",
    "output": "def bootstrap(config, repo_name, embeddings_model=None):\n    \n    if embeddings_model is None:\n        embeddings_model = Embeddings(\n            model=EmbeddingsModel[config[\"embeddings\"].upper().replace(\"-\", \"_\")],\n            deployment=(\n                config[\"embeddings-deployment\"]\n                if \"embeddings-deployment\" in config\n                else None\n            ),\n        )\n\n    vector_store = VectorStore(repo_name, embeddings=embeddings_model.embeddings)\n    vector_store.load_documents()\n\n    llm = LLM(\n        llm_host=LlmHost[config[\"llm-host\"].upper().replace(\"-\", \"_\")],\n        chat_model=config[\"chat-model\"],\n        deployment=config[\"model-deployment\"] if \"model-deployment\" in config else None,\n    )\n    memory = ConversationSummaryMemory(\n        llm=llm.chat_model, memory_key=\"chat_history\", return_messages=True\n    )\n    qa = ConversationalRetrievalChain.from_llm(\n        llm.chat_model, retriever=vector_store.retriever, memory=memory\n    )\n\n    return vector_store, memory, qa"
  },
  {
    "input": "\"\"\"\n    Returns the configuration file path based on the operating system.\n\n    This function determines the appropriate configuration directory based on the operating system\n    and constructs the full path to the configuration file.\n\n    Returns:\n        str: The path to the configuration file.\n\n    Raises:\n        NotImplementedError: If the operating system is not supported.\n    \"\"\"",
    "output": "def get_config_path():\n    \n    system = platform.system()\n\n    if system == \"Linux\" or system == \"Darwin\":\n        user_home = os.path.expanduser(\"~\")\n        config_dir = os.path.join(user_home, \".config\", \"codeqai\")\n    elif system == \"Windows\":\n        user_home = os.path.expanduser(\"~\")\n        config_dir = os.path.join(user_home, \"AppData\", \"Roaming\", \"codeqai\")\n    else:\n        raise NotImplementedError(f\"Unsupported platform: {system}\")\n\n    config_file_path = os.path.join(config_dir, \"config.yaml\")\n\n    return config_file_path"
  },
  {
    "input": "\"\"\"\n    Loads the configuration from the configuration file.\n\n    This function reads the configuration file specified by get_config_path() and parses its content\n    using the YAML parser.\n\n    Returns:\n        dict: The configuration dictionary loaded from the file.\n    \"\"\"",
    "output": "def load_config():\n    \n    with open(get_config_path(), \"r\", encoding=\"utf-8\") as config_file:\n        config = yaml.safe_load(config_file)\n    return config"
  },
  {
    "input": "\"\"\"\n    Saves the configuration to the configuration file.\n\n    Args:\n        config (dict): The configuration dictionary to be saved.\n\n    This function writes the provided configuration dictionary to the configuration file specified by get_config_path()\n    using the YAML format.\n    \"\"\"",
    "output": "def save_config(config):\n    \n    with open(get_config_path(), \"w\", encoding=\"utf-8\") as config_file:\n        yaml.dump(config, config_file, default_flow_style=False)"
  },
  {
    "input": "\"\"\"\n    Creates a new configuration interactively by prompting the user for input.\n\n    This function prompts the user with a series of questions to configure the embeddings model and LLM host.\n    Based on the user's responses, it constructs a configuration dictionary and saves it to the configuration file.\n\n    Returns:\n        dict: The configuration dictionary created based on user input.\n    \"\"\"",
    "output": "def create_config():\n    \n    os.makedirs(os.path.dirname(get_config_path()), exist_ok=True)\n\n    questions = [\n        inquirer.Confirm(\n            \"confirm\",\n            message=\"Do you want to use local embedding models?\",\n            default=False,\n        ),\n    ]\n\n    confirm = inquirer.prompt(questions)\n\n    if confirm and confirm[\"confirm\"]:\n        questions = [\n            inquirer.List(\n                \"embeddings\",\n                message=\"Which local embeddings model do you want to use?\",\n                choices=[\n                    EmbeddingsModel.INSTRUCTOR_LARGE.value,\n                    EmbeddingsModel.SENTENCETRANSFORMERS_ALL_MPNET_BASE_V2.value,\n                    EmbeddingsModel.SENTENCETRANSFORMERS_ALL_MINILM_L6_V2.value,\n                ],\n                default=EmbeddingsModel.INSTRUCTOR_LARGE.value,\n            ),\n        ]\n    else:\n        questions = [\n            inquirer.List(\n                \"embeddings\",\n                message=\"Which remote embeddings do you want to use?\",\n                choices=[\n                    EmbeddingsModel.OPENAI_TEXT_EMBEDDING_ADA_002.value,\n                    EmbeddingsModel.AZURE_OPENAI.value,\n                ],\n                default=EmbeddingsModel.OPENAI_TEXT_EMBEDDING_ADA_002.value,\n            ),\n        ]\n\n    answersEmbedding = inquirer.prompt(questions)\n\n    questions = [\n        inquirer.Confirm(\n            \"confirm\", message=\"Do you want to use local chat models?\", default=False\n        ),\n    ]\n\n    confirm = inquirer.prompt(questions)\n\n    if confirm and confirm[\"confirm\"]:\n        questions = [\n            inquirer.List(\n                \"llm-host\",\n                message=\"Which local LLM host do you want to use?\",\n                choices=[\n                    LlmHost.LLAMACPP.value,\n                    LlmHost.OLLAMA.value,\n                ],\n                default=LlmHost.LLAMACPP.value,\n            ),\n        ]\n    else:\n        questions = [\n            inquirer.List(\n                \"llm-host\",\n                message=\"Which remote LLM provider do you want to use?\",\n                choices=[\n                    LlmHost.OPENAI.value,\n                    LlmHost.AZURE_OPENAI.value,\n                    LlmHost.ANTHROPIC.value,\n                ],\n                default=LlmHost.OPENAI.value,\n            ),\n        ]\n\n    answersLlm = inquirer.prompt(questions)\n\n    if confirm and answersEmbedding and answersLlm:\n        config = {\n            \"embeddings\": answersEmbedding[\"embeddings\"],\n            \"llm-host\": answersLlm[\"llm-host\"],\n        }\n\n        if config[\"embeddings\"] == EmbeddingsModel.AZURE_OPENAI.value:\n            questions = [\n                inquirer.Text(\n                    \"deployment\",\n                    message=\"Please enter the Azure OpenAI embeddings deployment name.\",\n                    default=\"\",\n                ),\n            ]\n            deployment_answer = inquirer.prompt(questions)\n            if deployment_answer and deployment_answer[\"deployment\"]:\n                config[\"embeddings-deployment\"] = deployment_answer[\"deployment\"]\n\n        if config[\"llm-host\"] == LlmHost.AZURE_OPENAI.value:\n            questions = [\n                inquirer.Text(\n                    \"deployment\",\n                    message=\"Please enter the Azure OpenAI model deployment name\",\n                    default=\"\",\n                ),\n            ]\n            deployment_answer = inquirer.prompt(questions)\n            if deployment_answer and deployment_answer[\"deployment\"]:\n                config[\"model-deployment\"] = deployment_answer[\"deployment\"]\n                config[\"chat-model\"] = deployment_answer[\"deployment\"]\n\n        elif config[\"llm-host\"] == LlmHost.LLAMACPP.value:\n            questions = [\n                inquirer.Text(\n                    \"chat-model\",\n                    message=\"Please enter the path to the LLM\",\n                    default=\"\",\n                ),\n            ]\n\n        elif config[\"llm-host\"] == LlmHost.OLLAMA.value:\n            questions = [\n                inquirer.List(\n                    \"chat-model\",\n                    message=\"Which model do you want to use with Ollama?\",\n                    choices=[\n                        \"llama2:7b\",\n                        \"llama2:13b\",\n                        \"llama3.2:1b\",\n                        \"llama3.2:3b\",\n                        \"llama3.1:8b\",\n                        \"codellama:7b\",\n                        \"codellama:13b\",\n                        \"gemma2:9b\",\n                        \"gemma2:2b\",\n                        \"deepseek-r1:1.5b\",\n                        \"deepseek-r1:7b\",\n                        \"deepseek-r1:8b\",\n                        \"qwen2.5-coder:7b\",\n                        \"qwen2.5-coder:3b\",\n                    ],\n                    default=\"llama2:13b\",\n                ),\n            ]\n\n        elif config[\"llm-host\"] == LlmHost.OPENAI.value:\n            questions = [\n                inquirer.List(\n                    \"chat-model\",\n                    message=\"Which OpenAI model do you want to use?\",\n                    choices=[\n                        \"gpt-3.5-turbo\",\n                        \"gpt-4\",\n                        \"gpt-4-turbo\",\n                        \"gpt-4o\",\n                        \"gpt-4o-mini\",\n                        \"o1\",\n                        \"o1-mini\",\n                        \"o3-mini\",\n                    ],\n                    default=\"gpt-3.5-turbo\",\n                ),\n            ]\n\n        elif config[\"llm-host\"] == LlmHost.ANTHROPIC.value:\n            questions = [\n                inquirer.List(\n                    \"chat-model\",\n                    message=\"Which Anthropic model do you want to use?\",\n                    choices=[\n                        \"claude-3-opus-latest\",\n                        \"claude-3-5-sonnet-latest\",\n                        \"claude-3-5-haiku-latest\",\n                    ],\n                    default=\"claude-3-opus-latest\",\n                ),\n            ]\n\n        # Check if \"chat-model\" is already present in the case of Azure_OpenAI\n        if \"chat-model\" not in config:\n            answersChatmodel = inquirer.prompt(questions)\n            if answersChatmodel and answersChatmodel[\"chat-model\"]:\n                config[\"chat-model\"] = answersChatmodel[\"chat-model\"]\n\n        save_config(config)\n\n        return config\n\n    return {}"
  },
  {
    "input": "\"\"\"\n        Loads documents into the vector store.\n\n        This method reads the serialized FAISS index from a file, deserializes it, and loads it into the FAISS database.\n        It also loads the vector cache from a JSON file and initializes the retriever with the specified search parameters.\n        \"\"\"",
    "output": "def load_documents(self):\n        \n        with open(\n            os.path.join(get_cache_path(), f\"{self.name}.faiss.bytes\"), \"rb\"\n        ) as file:\n            index = file.read()\n\n        self.db = FAISS.deserialize_from_bytes(\n            embeddings=self.embeddings, serialized=index\n        )\n        self.vector_cache = load_vector_cache(f\"{self.name}.json\")\n        self.retriever = self.db.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 8})"
  },
  {
    "input": "\"\"\"\n        Indexes the given documents and stores them in the vector store.\n\n        This method creates a FAISS index from the provided documents and serializes it to a file.\n        It also creates a vector cache for quick lookup of document vectors and initializes the retriever.\n\n        Args:\n            documents (list[Document]): A list of Document objects to be indexed.\n        \"\"\"",
    "output": "def index_documents(self, documents: list[Document]):\n        \n        self.vector_cache = {}\n        self.db = FAISS.from_documents(documents, self.embeddings)\n        index = self.db.serialize_to_bytes()\n        with open(\n            os.path.join(get_cache_path(), f\"{self.name}.faiss.bytes\"), \"wb\"\n        ) as binary_file:\n            binary_file.write(index)\n        # Create vector cache\n        index_to_docstore_id = self.db.index_to_docstore_id\n        for i in range(len(documents)):\n            document = self.db.docstore.search(index_to_docstore_id[i])\n            if document and document is type(Document):\n                # Check if the document is already present in the vector cache\n                # if yes, then add the vector id to the vector cache entry\n                if self.vector_cache.get(document.metadata[\"filename\"]):\n                    self.vector_cache[document.metadata[\"filename\"]].vector_ids.append(\n                        index_to_docstore_id[i]\n                    )\n                # if no, then create a new entry in the vector cache\n                else:\n                    self.vector_cache[document.metadata[\"filename\"]] = VectorCache(\n                        document.metadata[\"filename\"],\n                        [index_to_docstore_id[i]],\n                        document.metadata[\"commit_hash\"],\n                    )\n\n        self.retriever = self.db.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 8})"
  },
  {
    "input": "\"\"\"\n        Synchronizes the documents in the vector store with the provided files.\n\n        This method checks if the documents in the vector store are up-to-date with the provided files.\n        If a document has been modified, it deletes the old vectors and adds new vectors.\n        If a document is new, it adds the document to the vector store.\n        It also removes old documents that are no longer present in the provided files.\n\n        Args:\n            files (list[str]): List of file paths to synchronize with the vector store.\n        \"\"\"",
    "output": "def sync_documents(self, files):\n        \n        new_filenames = set()\n        for file in files:\n            filename = os.path.basename(file)\n            new_filenames.add(filename)\n            commit_hash = get_commit_hash(file)\n            # Check if the document is already present in the vector cache\n            # if yes, then check if the document has been modified or not\n            if filename in self.vector_cache:\n                # Check if the document has been modified, if yes delete all old vectors and add new vector\n                if self.vector_cache[filename].commit_hash != commit_hash:\n                    # This will delete all the vectors associated with the document\n                    # incluing db.index_to_docstore_id, db.docstore and db.index\n                    try:\n                        self.db.delete(self.vector_cache[filename].vector_ids)\n                    except Exception as e:\n                        print(f\"Error deleting vectors for file {filename}: {e}\")\n\n                    # Add the new document to the vector store and recreate the vector cache entry\n                    self.vector_cache[filename] = VectorCache(\n                        filename,\n                        [],\n                        commit_hash,\n                    )\n                    documents = parse_code_files_for_db([file])\n                    for document in documents:\n                        self.db.add_documents([document])\n                        self.vector_cache[filename].vector_ids.append(\n                            self.db.index_to_docstore_id[\n                                len(self.db.index_to_docstore_id) - 1\n                            ]\n                        )\n\n            # if no, then create a new entry in the vector cache and add the document to the vector store\n            else:\n                self.vector_cache[filename] = VectorCache(\n                    filename,\n                    [],\n                    commit_hash,\n                )\n                documents = parse_code_files_for_db([file])\n                for document in documents:\n                    self.db.add_documents([document])\n                    self.vector_cache[filename].vector_ids.append(\n                        self.db.index_to_docstore_id[\n                            len(self.db.index_to_docstore_id) - 1\n                        ]\n                    )\n\n        # Remove old documents from the vector store\n        deleted_files = []\n        for cache_item in self.vector_cache.values():\n            if cache_item.filename not in new_filenames:\n                try:\n                    self.db.delete(cache_item.vector_ids)\n                except Exception as e:\n                    print(f\"Error deleting vectors for file {cache_item.filename}: {e}\")\n                deleted_files.append(cache_item.filename)\n\n        # Remove old filenames from the vector cache\n        for deleted_file in deleted_files:\n            self.vector_cache.pop(deleted_file)\n\n        index = self.db.serialize_to_bytes()\n        with open(\n            os.path.join(get_cache_path(), f\"{self.name}.faiss.bytes\"), \"wb\"\n        ) as binary_file:\n            binary_file.write(index)"
  },
  {
    "input": "\"\"\"\n        Exports the dataset based on the specified format.\n\n        This method checks the format of the dataset and calls the appropriate export method.\n        It also prints messages indicating the progress and completion of the export process.\n\n        Supported formats:\n        - CONVERSATIONAL: Exports to conversational_dataset.json\n        - ALPACA: Exports to alpaca_dataset.json\n        - INSTRUCTION: Exports to instruction_dataset.json\n        \"\"\"",
    "output": "def export(self):\n        \n        print(\"Exporting dataset...\")\n        if self.format == DatasetFormat.CONVERSATIONAL.value:\n            self.export_conversational()\n            print(\"Dataset exported to conversational_dataset.json\")\n        elif self.format == DatasetFormat.ALPACA.value:\n            self.export_alpaca()\n            print(\"Dataset exported to alpaca_dataset.json\")\n        elif self.format == DatasetFormat.INSTRUCTION.value:\n            self.export_instruction()\n            print(\"Dataset exported to instruction_dataset.json\")\n        elif self.format == DatasetFormat.COMPLETION.value:\n            self.export_completion()\n            print(\"Dataset exported to completion_dataset.json\")"
  },
  {
    "input": "\"\"\"\n        Exports the code snippets in a conversational format.\n\n        This method processes each code snippet in the dataset and creates conversational messages\n        for both implementation and explanation tasks. The messages are then saved to a JSON file.\n        \"\"\"",
    "output": "def export_conversational(self):\n        \n        messages_list = []\n        for code_snippet in self.code_snippets:\n            if code_snippet.get(\"description\") is None:\n                if (\n                    self.distillation_mode == DistillationMode.DOCUMENTATION\n                    or self.distillation_mode == DistillationMode.FULL\n                ):\n                    docstring = self.distill_docstring(code_snippet)\n                else:\n                    continue\n            else:\n                docstring = code_snippet.get(\"description\")\n\n            if (\n                self.distillation_mode == DistillationMode.CODE\n                or self.distillation_mode == DistillationMode.FULL\n            ):\n                pass\n\n            message = {\n                \"messages\": [\n                    {\n                        \"role\": \"system\",\n                        \"content\": \"You are a \"\n                        + (code_snippet.get(\"language\") or \"programming\")\n                        + \" expert. Write an implementation for the following description.\",\n                    },\n                    {\"role\": \"user\", \"content\": docstring},\n                    {\"role\": \"assistant\", \"content\": code_snippet.get(\"code\")},\n                ]\n            }\n\n            messages_list.append(message)\n            message = {\n                \"messages\": [\n                    {\n                        \"role\": \"system\",\n                        \"content\": \"You are a \"\n                        + (code_snippet.get(\"language\") or \"programming\")\n                        + \" expert. Explain the following code.\",\n                    },\n                    {\"role\": \"user\", \"content\": code_snippet.get(\"code\")},\n                    {\n                        \"role\": \"assistant\",\n                        \"content\": docstring,\n                    },\n                ]\n            }\n\n            messages_list.append(message)\n\n        with open(\"conversational_dataset.json\", \"w\") as f:\n            for messages in messages_list:\n                json.dump(messages, f)\n                f.write(\"\\n\")"
  },
  {
    "input": "\"\"\"\n        Exports the code snippets in an Alpaca format.\n\n        This method processes each code snippet in the dataset and creates Alpaca entries\n        for both implementation and explanation tasks. The entries are then saved to a JSON file.\n        \"\"\"",
    "output": "def export_alpaca(self):\n        \n        alpaca_list = []\n        for code_snippet in self.code_snippets:\n            if code_snippet.get(\"description\") is None:\n                if (\n                    self.distillation_mode == DistillationMode.DOCUMENTATION\n                    or self.distillation_mode == DistillationMode.FULL\n                ):\n                    docstring = self.distill_docstring(code_snippet)\n                else:\n                    continue\n            else:\n                docstring = code_snippet.get(\"description\")\n\n            if (\n                self.distillation_mode == DistillationMode.CODE\n                or self.distillation_mode == DistillationMode.FULL\n            ):\n                pass\n\n            alpaca_entry = {\n                \"instruction\": \"You are a \"\n                + (code_snippet.get(\"language\") or \"programming\")\n                + \" expert. Write an implementation for the following description.\",\n                \"input\": docstring,\n                \"output\": code_snippet.get(\"code\"),\n            }\n            alpaca_list.append(alpaca_entry)\n            alpaca_entry = {\n                \"instruction\": \"You are a \"\n                + (code_snippet.get(\"language\") or \"programming\")\n                + \" expert. Explain the following code.\",\n                \"input\": code_snippet.get(\"code\"),\n                \"output\": docstring,\n            }\n            alpaca_list.append(alpaca_entry)\n\n        with open(\"alpaca_dataset.json\", \"w\") as f:\n            json.dump(alpaca_list, f, indent=4)"
  },
  {
    "input": "\"\"\"\n        Exports the code snippets in an instruction format.\n\n        This method processes each code snippet in the dataset and creates instruction entries\n        for both implementation and explanation tasks. The entries are then saved to a JSON file.\n        \"\"\"",
    "output": "def export_instruction(self):\n        \n        instructions_list = []\n        for code_snippet in self.code_snippets:\n            if code_snippet.get(\"description\") is None:\n                if (\n                    self.distillation_mode == DistillationMode.DOCUMENTATION\n                    or self.distillation_mode == DistillationMode.FULL\n                ):\n                    result = self.distill_docstring(code_snippet)\n                    if type(result) is str:\n                        docstring = result\n                    else:\n                        continue\n                else:\n                    continue\n            else:\n                docstring = code_snippet.get(\"description\")\n\n            if (\n                self.distillation_mode == DistillationMode.CODE\n                or self.distillation_mode == DistillationMode.FULL\n            ):\n                pass\n\n            instruction = {\n                \"prompt\": \"You are a \"\n                + (code_snippet.get(\"language\") or \"programming\")\n                + \" expert. Write an implementation for the following description:\\n\"\n                + (docstring or \"\"),\n                \"completion\": code_snippet.get(\"code\"),\n            }\n            instructions_list.append(instruction)\n            instruction = {\n                \"prompt\": \"You are a \"\n                + (code_snippet.get(\"language\") or \"programming\")\n                + \" expert. Explain the following code:\\n\"\n                + (code_snippet.get(\"code\") or \"\"),\n                \"completion\": docstring,\n            }\n            instructions_list.append(instruction)\n\n        with open(\"instruction_dataset.json\", \"w\") as f:\n            json.dump(instructions_list, f, indent=4)"
  },
  {
    "input": "\"\"\"\n        Exports the code snippets in a completion format.\n\n        This method processes each code snippet in the dataset and creates completion entries.\n        The entries are then saved to a JSON file.\n        \"\"\"",
    "output": "def export_completion(self):\n        \n        completions_list = []\n        for code_snippet in self.code_snippets:\n            if code_snippet.get(\"description\") is None:\n                if (\n                    self.distillation_mode == DistillationMode.DOCUMENTATION\n                    or self.distillation_mode == DistillationMode.FULL\n                ):\n                    docstring = self.distill_docstring(code_snippet)\n                else:\n                    continue\n            else:\n                docstring = code_snippet.get(\"description\")\n\n            if (\n                self.distillation_mode == DistillationMode.CODE\n                or self.distillation_mode == DistillationMode.FULL\n            ):\n                pass\n\n            completion = {\n                \"input\": docstring,\n                \"output\": code_snippet.get(\"code\"),\n            }\n            completions_list.append(completion)\n\n        with open(\"completion_dataset.json\", \"w\") as f:\n            json.dump(completions_list, f, indent=4)"
  },
  {
    "input": "\"\"\"\n        Distills a concise description from the given code snippet.\n\n        Args:\n            code_snippet (dict): A dictionary containing details about the code snippet, such as method name, programming language, and the actual code.\n\n        Returns:\n            str: A concise description of the code snippet.\n        \"\"\"",
    "output": "def distill_docstring(self, code_snippet):\n        \n        spinner = yaspin(\n            text=f\"Distilling {code_snippet.get('method_name')}...\",\n            color=\"green\",\n        )\n        spinner.start()\n        prompt = (\n            \"You are a \"\n            + (code_snippet.get(\"language\") or \"programming\")\n            + \" expert. Write a short and concise description for the following code. Return only the description.\"\n        )\n        docstring = self.llm.chat_model.invoke(\n            [\n                (\"system\", prompt),\n                (\"human\", code_snippet.get(\"code\") or \"\"),\n            ]\n        )\n        spinner.stop()\n        return docstring.content"
  },
  {
    "input": "\"\"\"\n        Distills a given code snippet into smaller chunks and provides explanations for each chunk.\n\n        Args:\n            code_snippet (dict): A dictionary containing details about the code snippet, such as method name, programming language, and the actual code.\n\n        Returns:\n            dict: A dictionary containing the distilled code chunks and their explanations.\n        \"\"\"",
    "output": "def distill_code(self, code_snippet):\n        \n        spinner = yaspin(\n            text=f\"Distilling {code_snippet.get('method_name')}...\",\n            color=\"green\",\n        )\n        spinner.start()\n        prompt = (\n            \"You are a \"\n            + (code_snippet.get(\"language\") or \"programming\")\n            + \" expert. Split the following code into reasonable chunks and explain each chunk. \"\n            + \"Return a JSON object with a list of objects containing the code chunk with key 'code' and the explanation with key 'explanation'.\"\n        )\n        code = self.llm.chat_model.invoke(\n            [\n                (\"system\", prompt),\n                (\"human\", code_snippet.get(\"code\") or \"\"),\n            ]\n        )\n        spinner.stop()\n\n        try:\n            # Ensure the content is a string before parsing\n            if isinstance(code.content, str):\n                # Parse the output to a JSON object\n                code_json = json.loads(code.content)\n            else:\n                raise ValueError(\"Content is not a valid JSON string\")\n        except (json.JSONDecodeError, ValueError) as e:\n            # Handle JSON parsing error\n            print(f\"Error parsing response JSON: {e}\")\n            return {}\n\n        return code_json"
  },
  {
    "input": "\"\"\"\n    Loads a vector cache from a JSON file.\n\n    Args:\n        filename (str): The name of the file containing the vector cache.\n\n    Returns:\n        Dict[str, VectorCache]: A dictionary where the keys are strings and the values are VectorCache objects.\n    \"\"\"",
    "output": "def load_vector_cache(filename) -> Dict[str, VectorCache]:\n    \n    with open(\n        get_cache_path() + \"/\" + filename, \"r\", encoding=\"utf-8\"\n    ) as vector_cache_file:\n        vector_cache_json = json.load(vector_cache_file)\n    vector_cache = {}\n    for key, value in vector_cache_json.items():\n        vector_cache[key] = VectorCache.from_json(value)\n    return vector_cache"
  },
  {
    "input": "\"\"\"\n    Saves a vector cache to a JSON file.\n\n    Args:\n        vector_cache (Dict[str, VectorCache]): A dictionary where the keys are strings and the values are VectorCache objects.\n        filename (str): The name of the file to save the vector cache to.\n    \"\"\"",
    "output": "def save_vector_cache(vector_cache, filename):\n    \n    with open(\n        get_cache_path() + \"/\" + filename, \"w\", encoding=\"utf-8\"\n    ) as vector_cache_file:\n        json.dump(vector_cache, default=VectorCache.to_json, fp=vector_cache_file)"
  },
  {
    "input": "\"\"\"\n    Returns the cache directory path based on the operating system.\n\n    Returns:\n        str: The path to the cache directory.\n\n    Raises:\n        NotImplementedError: If the operating system is not supported.\n    \"\"\"",
    "output": "def get_cache_path():\n    \n    system = platform.system()\n\n    if system == \"Linux\" or system == \"Darwin\":\n        user_home = os.path.expanduser(\"~\")\n        cache_dir = os.path.join(user_home, \".cache\", \"codeqai\")\n    elif system == \"Windows\":\n        user_home = os.path.expanduser(\"~\")\n        cache_dir = os.path.join(user_home, \"AppData\", \"Local\", \"codeqai\")\n    else:\n        raise NotImplementedError(f\"Unsupported platform: {system}\")\n\n    return cache_dir"
  },
  {
    "input": "\"\"\"\n    Creates the cache directory if it does not already exist.\n\n    This function checks if the cache directory exists at the path returned by get_cache_path().\n    If the directory does not exist, it creates the directory and any necessary parent directories.\n    \"\"\"",
    "output": "def create_cache_dir():\n    \n    if not os.path.exists(get_cache_path()):\n        path = Path(get_cache_path())\n        path.mkdir(parents=True, exist_ok=True)"
  },
  {
    "input": "\"\"\"\n        Initializes the LLM class with the specified parameters.\n\n        Args:\n            llm_host (LlmHost): The host for the language model (e.g., OPENAI, AZURE_OPENAI, ANTHROPIC, LLAMACPP, OLLAMA).\n            chat_model (str): The chat model to use.\n            max_tokens (int, optional): The maximum number of tokens for the model. Defaults to 2048.\n            deployment (str, optional): The deployment name for Azure OpenAI. Defaults to None.\n\n        Raises:\n            ValueError: If the required environment variable for Azure OpenAI is not set.\n        \"\"\"",
    "output": "def __init__(\n        self, llm_host: LlmHost, chat_model: str, max_tokens=2048, deployment=None\n    ):\n        \n        if llm_host == LlmHost.OPENAI:\n            self.chat_model = ChatOpenAI(\n                temperature=0.9, max_tokens=max_tokens, model=chat_model\n            )\n        elif llm_host == LlmHost.AZURE_OPENAI and deployment:\n            azure_openai_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n            if azure_openai_endpoint:\n                self.chat_model = AzureChatOpenAI(\n                    azure_endpoint=azure_openai_endpoint,\n                    temperature=0.9,\n                    max_tokens=max_tokens,\n                    model=chat_model,\n                )\n            else:\n                raise ValueError(\n                    \"Azure OpenAI requires environment variable AZURE_OPENAI_ENDPOINT to be set.\"\n                )\n        elif llm_host == LlmHost.ANTHROPIC:\n            self.chat_model = ChatAnthropic(\n                temperature=0.9,\n                max_tokens_to_sample=max_tokens,\n                model_name=chat_model,\n                timeout=30,\n                api_key=None,  # API key is set to environment variable ANTHROPIC_API_KEY\n            )\n        elif llm_host == LlmHost.LLAMACPP:\n            self.install_llama_cpp()\n            self.chat_model = LlamaCpp(\n                model_path=chat_model,\n                temperature=0.9,\n                max_tokens=max_tokens,\n                verbose=False,\n            )\n        elif llm_host == LlmHost.OLLAMA:\n            self.chat_model = Ollama(\n                base_url=\"http://localhost:11434\",\n                model=chat_model,\n                callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),\n            )"
  },
  {
    "input": "\"\"\"\n        Initializes the Embeddings class with the specified model and deployment.\n\n        Args:\n            model (EmbeddingsModel): The embeddings model to use. Defaults to OPENAI_TEXT_EMBEDDING_ADA_002.\n            deployment (str, optional): The deployment name for Azure OpenAI embeddings. Defaults to None.\n        \"\"\"",
    "output": "def __init__(\n        self,\n        model=EmbeddingsModel.OPENAI_TEXT_EMBEDDING_ADA_002,\n        deployment=None,\n    ):\n        \n        if model == EmbeddingsModel.OPENAI_TEXT_EMBEDDING_ADA_002:\n            self.embeddings = OpenAIEmbeddings(\n                client=None, model=\"text-embedding-ada-002\"\n            )\n        elif model == EmbeddingsModel.AZURE_OPENAI and deployment:\n            self.embeddings = AzureOpenAIEmbeddings(\n                client=None, azure_deployment=deployment\n            )\n        else:\n            try:\n                import sentence_transformers  # noqa: F401\n            except ImportError:\n                self._install_sentence_transformers()\n\n            if model == EmbeddingsModel.SENTENCETRANSFORMERS_ALL_MPNET_BASE_V2:\n                self.embeddings = HuggingFaceEmbeddings()\n            elif model == EmbeddingsModel.SENTENCETRANSFORMERS_ALL_MINILM_L6_V2:\n                self.embeddings = HuggingFaceEmbeddings(\n                    model_name=EmbeddingsModel.SENTENCETRANSFORMERS_ALL_MINILM_L6_V2.value.replace(\n                        \"SentenceTransformers-\", \"\"\n                    )\n                )\n            elif model == EmbeddingsModel.INSTRUCTOR_LARGE:\n                try:\n                    from InstructorEmbedding import INSTRUCTOR  # noqa: F401\n                except ImportError:\n                    self._install_instructor_embedding()\n\n                self.embeddings = HuggingFaceEmbeddings(\n                    model_name=\"hkunlp/instructor-xl\"\n                )"
  },
  {
    "input": "\"\"\"\n    Returns the programming language based on the provided file extension.\n\n    Args:\n        file_extension (str): The file extension to determine the programming language of.\n\n    Returns:\n        Language: The programming language corresponding to the file extension. If the file extension is not found\n        in the language mapping, returns Language.UNKNOWN.\n    \"\"\"",
    "output": "def get_programming_language(file_extension: str) -> Language:\n    \n    language_mapping = {\n        \".py\": Language.PYTHON,\n        \".js\": Language.JAVASCRIPT,\n        \".jsx\": Language.JAVASCRIPT,\n        \".mjs\": Language.JAVASCRIPT,\n        \".cjs\": Language.JAVASCRIPT,\n        \".ts\": Language.TYPESCRIPT,\n        \".tsx\": Language.TYPESCRIPT,\n        \".java\": Language.JAVA,\n        \".kt\": Language.KOTLIN,\n        \".rs\": Language.RUST,\n        \".go\": Language.GO,\n        \".cpp\": Language.CPP,\n        \".c\": Language.C,\n        \".cs\": Language.C_SHARP,\n        \".hs\": Language.HASKELL,\n        \".rb\": Language.RUBY,\n    }\n    return language_mapping.get(file_extension, Language.UNKNOWN)"
  },
  {
    "input": "\"\"\"\n    Returns the extension of a file from its given name.\n\n    Parameters:\n        file_name (str): The name of the file.\n\n    Returns:\n        str: The extension of the file.\n\n    \"\"\"",
    "output": "def get_file_extension(file_name: str) -> str:\n    \n    return os.path.splitext(file_name)[-1]"
  },
  {
    "input": "\"\"\"\n    Returns the given text formatted in bold.\n\n    Args:\n        text (str): The text to be formatted.\n\n    Returns:\n        str: The text formatted in bold using ANSI escape codes.\n    \"\"\"",
    "output": "def get_bold_text(text):\n    \n    return f\"\\033[01m{text}\\033[0m\""
  },
  {
    "input": "\"\"\"\n    Finds the starting line number and indentation level of a code snippet within a file.\n\n    Args:\n        filename (str): The name of the file to search within.\n        code_snippet (str): The code snippet to find in the file.\n\n    Returns:\n        tuple: A tuple containing the starting line number (int) and the indentation level (str) of the code snippet.\n               If the file is not found or the code snippet is not found, returns (1, \"\").\n    \"\"\"",
    "output": "def find_starting_line_and_indent(filename, code_snippet):\n    \n    file_path = find_file_in_git_repo(filename)\n    if file_path is not None:\n        with open(file_path, \"r\") as file:\n            file_content = file.read()\n            start_pos = file_content.find(code_snippet)\n            return (\n                file_content.count(\"\\n\", 0, start_pos) + 1,\n                file_content[:start_pos].split(\"\\n\")[-1],\n            )\n    return 1, \"\""
  },
  {
    "input": "\"\"\"\n    Counts the number of tokens in the given text using the specified model's tokenizer.\n\n    Args:\n        text (str): The text to be tokenized and counted.\n        model (str, optional): The model to use for tokenization. Defaults to \"gpt-4\".\n\n    Returns:\n        int: The number of tokens in the text.\n    \"\"\"",
    "output": "def count_tokens(text, model=\"gpt-4\"):\n    \n    enc = tiktoken.encoding_for_model(model)\n    return len(enc.encode(text))"
  },
  {
    "input": "\"\"\"\n    Retrieves the name of the current Git repository.\n\n    This function gets the root directory of the current Git repository based on the current working directory,\n    and extracts the repository name from the root directory path.\n\n    Returns:\n        str: The name of the current Git repository.\n    \"\"\"",
    "output": "def repo_name():\n    \n    return get_git_root(os.getcwd()).split(\"/\")[-1]"
  },
  {
    "input": "\"\"\"\n    Retrieves the root directory of the Git repository for the given path.\n\n    Args:\n        path (str): The path to a directory within the Git repository.\n\n    Returns:\n        str: The root directory of the Git repository.\n    \"\"\"",
    "output": "def get_git_root(path):\n    \n    git_repo = Repo(path, search_parent_directories=True)\n    git_root = git_repo.git.rev_parse(\"--show-toplevel\")\n    return git_root"
  },
  {
    "input": "\"\"\"\n    Searches for a file with the given name in the current Git repository.\n\n    Args:\n        file_name (str): The name of the file to search for.\n\n    Returns:\n        str or None: The full path to the file if found, otherwise None.\n    \"\"\"",
    "output": "def find_file_in_git_repo(file_name):\n    \n    git_root = get_git_root(os.getcwd())\n\n    for root, dirs, files in os.walk(git_root):\n        if any(blacklist in root for blacklist in BLACKLIST_DIR):\n            continue\n        for file in files:\n            if file == file_name:\n                return os.path.join(root, file)"
  },
  {
    "input": "\"\"\"\n    Loads files from the current Git repository based on whitelist and blacklist criteria.\n\n    This function walks through the directory structure of the Git repository,\n    and collects files that match the whitelist extensions and are not in the blacklist directories or files.\n\n    Returns:\n        list: A list of file paths that meet the criteria.\n    \"\"\"",
    "output": "def load_files():\n    \n    git_root = get_git_root(os.getcwd())\n    file_list = []\n\n    for root, dirs, files in os.walk(git_root):\n        if any(blacklist in root for blacklist in BLACKLIST_DIR):\n            continue\n        for file in files:\n            file_ext = os.path.splitext(file)[1]\n            if any(whitelist == file_ext for whitelist in WHITELIST_FILES):\n                if file not in BLACKLIST_FILES:\n                    file_list.append(os.path.join(root, file))\n\n    return file_list"
  },
  {
    "input": "\"\"\"\n    Retrieves the latest commit hash for the specified file.\n\n    Args:\n        file_path (str): The path to the file for which to retrieve the commit hash.\n\n    Returns:\n        str or None: The latest commit hash if found, otherwise None.\n    \"\"\"",
    "output": "def get_commit_hash(file_path):\n    \n    try:\n        # Run the git log command\n        result = subprocess.run(\n            [\"git\", \"log\", \"-n\", \"1\", \"--pretty=format:%H\", \"--\", file_path],\n            stdout=subprocess.PIPE,\n            text=True,\n            check=True,\n        )\n\n        # Extract the commit hash from the command output\n        commit_hash = result.stdout.strip()\n        return commit_hash\n\n    except subprocess.CalledProcessError as e:\n        print(f\"Error executing git command: {e}\")\n        return None"
  },
  {
    "input": "\"\"\"\n    Args :\n    env_path = source path of .env file.\n    required_keys = [\"OPENAI_KEY\"] #change this according to need\n\n    #running/calling the function.\n    configs = env_loader('.env', required_keys)\n    \"\"\"",
    "output": "def env_loader(env_path, required_keys=None):\n    \n\n    # create env file if does not exists\n    # parse required keys in the file if it's not None\n    if not os.path.exists(env_path) or os.path.getsize(env_path) == 0:\n        with open(env_path, \"w\") as env_f:\n            if required_keys:\n                for key in required_keys:\n                    env_f.write(f'{key}=\"\"\\n')\n            else:\n                pass\n\n    configs = dotenv_values(env_path)\n    changed = False\n    for key, value in configs.items():\n        env_key = os.getenv(key)\n        if not value and not env_key:\n            value = input(\n                f\"[+] Key {utils.get_bold_text(key)} is required. Please enter it's value: \"\n            )\n            configs[key] = value\n            changed = True\n        elif not value and env_key:\n            value = env_key\n            configs[key] = value\n            changed = True\n\n    # update the .env file if config is changed/taken from user\n    if changed:\n        with open(env_path, \"w\") as env_f:\n            for key, value in configs.items():\n                env_f.write(f'{key}=\"{value}\"\\n')\n\n    load_dotenv(env_path, override=True)"
  },
  {
    "input": "\"\"\"\n    Parses a list of code files and returns a list of Document objects for database storage.\n\n    Args:\n        code_files (list[str]): List of paths to code files to be parsed.\n\n    Returns:\n        list[Document]: List of Document objects containing parsed code information.\n    \"\"\"",
    "output": "def parse_code_files_for_db(code_files: list[str]) -> list[Document]:\n    \n    documents = []\n    code_splitter = None\n    for code_file in code_files:\n        with open(code_file, \"r\", encoding=\"utf-8\") as file:\n            file_bytes = file.read().encode()\n            commit_hash = repo.get_commit_hash(code_file)\n\n            file_extension = utils.get_file_extension(code_file)\n            programming_language = utils.get_programming_language(file_extension)\n            if programming_language == Language.UNKNOWN:\n                continue\n\n            langchain_language = utils.get_langchain_language(programming_language)\n\n            if langchain_language:\n                code_splitter = RecursiveCharacterTextSplitter.from_language(\n                    language=langchain_language,\n                    chunk_size=512,\n                    chunk_overlap=128,\n                )\n\n            treesitter_parser = Treesitter.create_treesitter(programming_language)\n            treesitterNodes: list[TreesitterMethodNode] = treesitter_parser.parse(\n                file_bytes\n            )\n            for node in treesitterNodes:\n                method_source_code = node.method_source_code\n                filename = os.path.basename(code_file)\n\n                if node.doc_comment and programming_language != Language.PYTHON:\n                    method_source_code = node.doc_comment + \"\\n\" + method_source_code\n\n                splitted_documents = [method_source_code]\n                if code_splitter:\n                    splitted_documents = code_splitter.split_text(method_source_code)\n\n                for splitted_document in splitted_documents:\n                    document = Document(\n                        page_content=splitted_document,\n                        metadata={\n                            \"filename\": filename,\n                            \"method_name\": node.name,\n                            \"commit_hash\": commit_hash,\n                        },\n                    )\n                    documents.append(document)\n\n    return documents"
  },
  {
    "input": "\"\"\"\n    Parses a list of code files for fine-tuning and returns a list of dictionaries containing method information.\n\n    Args:\n        code_files (list[str]): List of paths to code files to be parsed.\n        max_tokens (int): Maximum number of tokens allowed for output.\n\n    Returns:\n        list[dict]: List of dictionaries containing method information, including method name, code, description, and language.\n    \"\"\"",
    "output": "def parse_code_files_for_finetuning(\n    code_files: list[str], max_tokens, spinner\n) -> list[dict]:\n    \n    input_tokens = 0\n    output_tokens = 0\n    documents = []\n    for code_file in code_files:\n        with open(code_file, \"r\", encoding=\"utf-8\") as file:\n            file_bytes = file.read().encode()\n\n            file_extension = utils.get_file_extension(code_file)\n            programming_language = utils.get_programming_language(file_extension)\n            if programming_language == Language.UNKNOWN:\n                continue\n\n            treesitter_parser = Treesitter.create_treesitter(programming_language)\n            treesitterNodes: list[TreesitterMethodNode] = treesitter_parser.parse(\n                file_bytes\n            )\n            for node in treesitterNodes:\n                method_source_code = node.method_source_code\n\n                if node.doc_comment and programming_language == Language.PYTHON:\n                    method_source_code = method_source_code.replace(\n                        node.doc_comment, \"\"\n                    )\n\n                document = {\n                    \"method_name\": node.name,\n                    \"code\": method_source_code,\n                    \"description\": node.doc_comment,\n                    \"language\": programming_language.value,\n                }\n                documents.append(document)\n\n                if node.doc_comment is not None:\n                    input_tokens += utils.count_tokens(node.doc_comment)\n                    output_tokens += max_tokens\n\n    spinner.stop()\n\n    print(f\"Estimated input tokens for distillation needed: {input_tokens}.\")\n    print(f\"Maximum output tokens for distillation nedeed: {output_tokens}.\")\n    questions = [\n        inquirer.Confirm(\n            \"confirm\",\n            message=\"Proceed?\",\n            default=True,\n        ),\n    ]\n\n    confirm = inquirer.prompt(questions)\n\n    if confirm and confirm[\"confirm\"]:\n        pass\n    else:\n        exit()\n\n    return documents"
  },
  {
    "input": "\"\"\"\n        Parses the given file bytes and extracts method nodes.\n\n        Args:\n            file_bytes (bytes): The content of the file to be parsed.\n\n        Returns:\n            list[TreesitterMethodNode]: A list of TreesitterMethodNode objects representing the methods in the file.\n        \"\"\"",
    "output": "def parse(self, file_bytes: bytes) -> list[TreesitterMethodNode]:\n        \n        self.tree = self.parser.parse(file_bytes)\n        result = []\n        methods = self._query_all_methods(self.tree.root_node)\n        for method in methods:\n            method_name = self._query_method_name(method)\n            doc_comment = self._query_doc_comment(method)\n            result.append(TreesitterMethodNode(method_name, doc_comment, None, method))\n        return result"
  },
  {
    "input": "\"\"\"\n        Queries the method name from the given syntax tree node.\n\n        Args:\n            node (tree_sitter.Node): The syntax tree node to query.\n\n        Returns:\n            str or None: The method name if found, otherwise None.\n        \"\"\"",
    "output": "def _query_method_name(self, node: tree_sitter.Node):\n        \n        if node.type == self.method_declaration_identifier:\n            for child in node.children:\n                if child.type == self.method_name_identifier:\n                    return child.text.decode()\n        return None"
  },
  {
    "input": "\"\"\"\n        Queries all method nodes within the given syntax tree node, including those within class definitions.\n\n        Args:\n            node (tree_sitter.Node): The root node to start the query from.\n\n        Returns:\n            list: A list of method nodes found within the given node.\n        \"\"\"",
    "output": "def _query_all_methods(self, node: tree_sitter.Node):\n        \n        methods = []\n        for child in node.children:\n            if child.type == self.method_declaration_identifier:\n                methods.append(child)\n            if child.type == \"class_definition\":\n                class_body = child.children[-1]\n                for child_node in class_body.children:\n                    if child_node.type == self.method_declaration_identifier:\n                        methods.append(child_node)\n        return methods"
  },
  {
    "input": "\"\"\"\n        Queries the documentation comment for the given function definition node.\n\n        Args:\n            node (tree_sitter.Node): The syntax tree node representing a function definition.\n\n        Returns:\n            str or None: The documentation comment string if found, otherwise None.\n        \"\"\"",
    "output": "def _query_doc_comment(self, node: tree_sitter.Node):\n        \n        query_code = \"\"\"\n            (function_definition\n                body: (block . (expression_statement (string)) @function_doc_str))\n        \"\"\"\n        doc_str_query = self.language.query(query_code)\n        doc_strs = doc_str_query.captures(node)\n\n        if doc_strs:\n            return doc_strs[0][0].text.decode()\n        else:\n            return None"
  },
  {
    "input": "\"\"\"\n        Queries the method name from the given syntax tree node.\n\n        Args:\n            node (tree_sitter.Node): The syntax tree node to query.\n\n        Returns:\n            str or None: The method name if found, otherwise None.\n        \"\"\"",
    "output": "def _query_method_name(self, node: tree_sitter.Node):\n        \n        first_match = None\n        if node.type == self.method_declaration_identifier:\n            for child in node.children:\n                # if the return type is an object type, then the method name\n                # is the second match\n                if child.type == self.method_name_identifier and not first_match:\n                    first_match = child.text.decode()\n                elif child.type == self.method_name_identifier and first_match:\n                    return child.text.decode()\n        return first_match"
  },
  {
    "input": "\"\"\"\n        Recursively queries all method nodes in the given syntax tree node.\n\n        Args:\n            node (tree_sitter.Node): The root node to start the query from.\n\n        Returns:\n            list: A list of dictionaries, each containing a method node and its associated doc comment (if any).\n        \"\"\"",
    "output": "def _query_all_methods(self, node: tree_sitter.Node):\n        \n        methods = []\n        if node.type == self.method_declaration_identifier:\n            doc_comment_nodes = []\n            if (\n                node.prev_named_sibling\n                and node.prev_named_sibling.type == self.doc_comment_identifier\n            ):\n                current_doc_comment_node = node.prev_named_sibling\n                while (\n                    current_doc_comment_node\n                    and current_doc_comment_node.type == self.doc_comment_identifier\n                ):\n                    doc_comment_nodes.append(current_doc_comment_node.text.decode())\n                    if current_doc_comment_node.prev_named_sibling:\n                        current_doc_comment_node = (\n                            current_doc_comment_node.prev_named_sibling\n                        )\n                    else:\n                        current_doc_comment_node = None\n\n            doc_comment_str = \"\"\n            doc_comment_nodes.reverse()\n            for doc_comment_node in doc_comment_nodes:\n                doc_comment_str += doc_comment_node + \"\\n\"\n            if doc_comment_str.strip() != \"\":\n                methods.append({\"method\": node, \"doc_comment\": doc_comment_str.strip()})\n            else:\n                methods.append({\"method\": node, \"doc_comment\": None})\n        else:\n            for child in node.children:\n                methods.extend(self._query_all_methods(child))\n        return methods"
  },
  {
    "input": "\"\"\"\n        Queries the method name from the given syntax tree node.\n\n        Args:\n            node (tree_sitter.Node): The syntax tree node to query.\n\n        Returns:\n            str or None: The method name if found, otherwise None.\n        \"\"\"",
    "output": "def _query_method_name(self, node: tree_sitter.Node):\n        \n        if node.type == self.method_declaration_identifier:\n            for child in node.children:\n                # if method returns pointer, skip pointer declarator\n                if child.type == \"pointer_declarator\":\n                    child = child.children[1]\n                if child.type == \"function_declarator\":\n                    for child in child.children:\n                        if child.type == self.method_name_identifier:\n                            return child.text.decode()\n        return None"
  },
  {
    "input": "\"\"\"\n        Recursively queries all method nodes in the given syntax tree node.\n\n        Args:\n            node (tree_sitter.Node): The root node to start the query from.\n\n        Returns:\n            list: A list of dictionaries, each containing a method node and its associated doc comment (if any).\n        \"\"\"",
    "output": "def _query_all_methods(self, node: tree_sitter.Node):\n        \n        methods = []\n        if node.type == self.method_declaration_identifier:\n            doc_comment_nodes = []\n            if (\n                node.prev_named_sibling\n                and node.prev_named_sibling.type == self.doc_comment_identifier\n            ):\n                current_doc_comment_node = node.prev_named_sibling\n                while (\n                    current_doc_comment_node\n                    and current_doc_comment_node.type == self.doc_comment_identifier\n                ):\n                    doc_comment_nodes.append(current_doc_comment_node.text.decode())\n                    if current_doc_comment_node.prev_named_sibling:\n                        current_doc_comment_node = (\n                            current_doc_comment_node.prev_named_sibling\n                        )\n                    else:\n                        current_doc_comment_node = None\n\n            doc_comment_str = \"\"\n            doc_comment_nodes.reverse()\n            for doc_comment_node in doc_comment_nodes:\n                doc_comment_str += doc_comment_node + \"\\n\"\n            if doc_comment_str.strip() != \"\":\n                methods.append({\"method\": node, \"doc_comment\": doc_comment_str.strip()})\n            else:\n                methods.append({\"method\": node, \"doc_comment\": None})\n        else:\n            for child in node.children:\n                methods.extend(self._query_all_methods(child))\n        return methods"
  },
  {
    "input": "\"\"\"\n        Queries the method name from the given syntax tree node.\n\n        Args:\n            node (tree_sitter.Node): The syntax tree node to query.\n\n        Returns:\n            str or None: The method name if found, otherwise None.\n        \"\"\"",
    "output": "def _query_method_name(self, node: tree_sitter.Node):\n        \n        if node.type == self.method_declaration_identifier:\n            for child in node.children:\n                # if method returns pointer, skip pointer declarator\n                if child.type == \"pointer_declarator\":\n                    child = child.children[1]\n                if child.type == \"function_declarator\":\n                    for child in child.children:\n                        if child.type == self.method_name_identifier:\n                            return child.text.decode()\n        return None"
  },
  {
    "input": "\"\"\"\n        Parses the given file bytes and extracts method nodes.\n\n        Args:\n            file_bytes (bytes): The content of the file to be parsed.\n\n        Returns:\n            list[TreesitterMethodNode]: A list of TreesitterMethodNode objects representing the methods in the file.\n        \"\"\"",
    "output": "def parse(self, file_bytes: bytes) -> list[TreesitterMethodNode]:\n        \n        self.tree = self.parser.parse(file_bytes)\n        result = []\n        methods = self._query_all_methods(self.tree.root_node)\n        for method in methods:\n            method_name = self._query_method_name(method[\"method\"])\n            doc_comment = method[\"doc_comment\"]\n            source_code = None\n            if method[\"method\"].type == \"signature\":\n                sc = map(\n                    lambda x: \"\\n\" + x.text.decode() if x.type == \"function\" else \"\",\n                    method[\"method\"].children,\n                )\n                source_code = method[\"method\"].text.decode() + \"\".join(sc)\n            result.append(\n                TreesitterMethodNode(\n                    method_name, doc_comment, source_code, method[\"method\"]\n                )\n            )\n        return result"
  },
  {
    "input": "\"\"\"\n        Recursively queries all method nodes in the given syntax tree node.\n\n        Args:\n            node (tree_sitter.Node): The root node to start the query from.\n\n        Returns:\n            list: A list of dictionaries, each containing a method node and its associated doc comment (if any).\n        \"\"\"",
    "output": "def _query_all_methods(\n        self,\n        node: tree_sitter.Node,\n    ):\n        \n        methods = []\n        if node.type == self.method_declaration_identifier:\n            doc_comment_node = None\n            if (\n                node.prev_named_sibling\n                and node.prev_named_sibling.type == self.doc_comment_identifier\n            ):\n                doc_comment_node = node.prev_named_sibling.text.decode()\n            else:\n                if (\n                    node.prev_named_sibling\n                    and node.prev_named_sibling.type == \"signature\"\n                ):\n                    prev_node = node.prev_named_sibling\n                    if (\n                        prev_node.prev_named_sibling\n                        and prev_node.prev_named_sibling.type\n                        == self.doc_comment_identifier\n                    ):\n                        doc_comment_node = prev_node.prev_named_sibling.text.decode()\n                    prev_node.children.append(node)\n                    node = prev_node\n            methods.append({\"method\": node, \"doc_comment\": doc_comment_node})\n        else:\n            for child in node.children:\n                current = self._query_all_methods(child)\n                if methods and current:\n                    previous = methods[-1]\n                    if self._query_method_name(\n                        previous[\"method\"]\n                    ) == self._query_method_name(current[0][\"method\"]):\n                        previous[\"method\"].children.extend(\n                            map(lambda x: x[\"method\"], current)\n                        )\n                        methods = methods[:-1]\n                        methods.append(previous)\n                    else:\n                        methods.extend(current)\n                else:\n                    methods.extend(current)\n        return methods"
  },
  {
    "input": "\"\"\"\n        Queries the method name from the given syntax tree node.\n\n        Args:\n            node (tree_sitter.Node): The syntax tree node to query.\n\n        Returns:\n            str or None: The method name if found, otherwise None.\n        \"\"\"",
    "output": "def _query_method_name(self, node: tree_sitter.Node):\n        \n        if node.type == \"signature\" or node.type == self.method_declaration_identifier:\n            for child in node.children:\n                if child.type == self.method_name_identifier:\n                    return child.text.decode()\n        return None"
  },
  {
    "input": "\"\"\"\n        Parses the given file bytes and extracts method nodes.\n\n        Args:\n            file_bytes (bytes): The content of the file to be parsed.\n\n        Returns:\n            list[TreesitterMethodNode]: A list of TreesitterMethodNode objects representing the methods in the file.\n        \"\"\"",
    "output": "def parse(self, file_bytes: bytes) -> list[TreesitterMethodNode]:\n        \n        self.tree = self.parser.parse(file_bytes)\n        result = []\n        methods = self._query_all_methods(self.tree.root_node)\n        for method in methods:\n            method_name = self._query_method_name(method[\"method\"])\n            doc_comment = method[\"doc_comment\"]\n            result.append(\n                TreesitterMethodNode(method_name, doc_comment, None, method[\"method\"])\n            )\n        return result"
  },
  {
    "input": "\"\"\"\n        Recursively queries all method nodes in the given syntax tree node.\n\n        Args:\n            node (tree_sitter.Node): The root node to start the query from.\n\n        Returns:\n            list: A list of dictionaries, each containing a method node and its associated doc comment (if any).\n        \"\"\"",
    "output": "def _query_all_methods(\n        self,\n        node: tree_sitter.Node,\n    ):\n        \n        methods = []\n        if node.type == self.method_declaration_identifier:\n            doc_comment_node = None\n            if (\n                node.prev_named_sibling\n                and node.prev_named_sibling.type == self.doc_comment_identifier\n            ):\n                doc_comment_node = node.prev_named_sibling.text.decode()\n            methods.append({\"method\": node, \"doc_comment\": doc_comment_node})\n        else:\n            for child in node.children:\n                methods.extend(self._query_all_methods(child))\n        return methods"
  },
  {
    "input": "\"\"\"\n        Queries the method name from the given syntax tree node.\n\n        Args:\n            node (tree_sitter.Node): The syntax tree node to query.\n\n        Returns:\n            str or None: The method name if found, otherwise None.\n        \"\"\"",
    "output": "def _query_method_name(self, node: tree_sitter.Node):\n        \n        if node.type == self.method_declaration_identifier:\n            for child in node.children:\n                if child.type == self.method_name_identifier:\n                    return child.text.decode()\n        return None"
  },
  {
    "input": "\"\"\"\n        Recursively queries all method nodes in the given syntax tree node.\n\n        Args:\n            node (tree_sitter.Node): The root node to start the query from.\n\n        Returns:\n            list: A list of dictionaries, each containing a method node and its associated doc comment (if any).\n        \"\"\"",
    "output": "def _query_all_methods(\n        self,\n        node: tree_sitter.Node,\n    ):\n        \n        methods = []\n        if node.type == self.method_declaration_identifier:\n            doc_comment = []\n            doc_comment_node = node\n            while (\n                doc_comment_node.prev_named_sibling\n                and doc_comment_node.prev_named_sibling.type\n                == self.doc_comment_identifier\n            ):\n                doc_comment_node = doc_comment_node.prev_named_sibling\n                doc_comment.insert(0, doc_comment_node.text.decode())\n            methods.append({\"method\": node, \"doc_comment\": \"\\n\".join(doc_comment)})\n        else:\n            for child in node.children:\n                methods.extend(self._query_all_methods(child))\n        return methods"
  }
]
